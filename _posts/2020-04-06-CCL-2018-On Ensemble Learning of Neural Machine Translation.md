---
title: CCL-2018-On Ensemble Learning of Neural Machine Translation
date: 2021-04-06
tags:
- 机器翻译
- 论文阅读
---

# 1 背景
实验室李北师兄的一篇论文，针对NMT中的模型集成进行了大量的实验对比。恰好我也想训练一个效果较好的中英互译模型，师兄的论文给了我一些启发，记录在此。

# 2 笔记
## 2.1 何为集成学习
是一种联合多个学习器进行协同决策的机器学习方法，通过整合多个学习器的决策结果可以有效减小预测结果的方差与偏置，显著提升模型的泛化能力，达到比单学习器更好的效果。

## 2.2 NMT中的模型集成方法
### 2.2.1 模型参数平均
即将单一模型最近保存的N个检查点的参数矩阵进行平均。

Vaswani等建议每隔10分钟保存一次模型，并平均最新保存的20个检查点。
### 2.2.2 预测结果融合
即在解码过程中，在经过Softmax得到归一化的词表上的概率分布后，整合不同模型得到的概率分布，进而预测下一个词。

由于不需要参数平均，那么模型的结构就不需要一致，这里又有两种手段构造子模型：

1. 使用相同模型结构，不同的参数初始化分布或不同的随机初始化种子
2. 使用不同的模型结构，不同的随机初始化种子

在数据层面，也可以通过finetuning和bagging两种策略构造子模型，这里暂不提及

## 2.3 实验步骤
### 2.3.1 数据筛选
使用WMT18中英数据，平行语料16M，英文单语24M。还用了18CWMT平行语料9M。（为啥我没找到QAQ，我就找到30万的news v15语料，可能我太笨了）

1. 对双语平行语料进行乱码过滤，剔除混有乱码的语料如控制字符、UTF-8转码生成的单字节乱码等
2. 对双语平行语料进行NiuTrans分词处理（师兄之前跟我说过实验室有自己的分词，难道就是这个），保留目标英文单词的大小写敏感，对中文端的标点符号进行全角转半角操作
3. 对双语语料进行长度比过滤，过滤源语端与目标语端超过100个词的句子，同时保证源语与目标语长度比在0.4~3.0范围内
4. 应用fast-align脚本对大规模双语语料做词对齐学习，在此基础上生成中英互译词典。清除源语与目标语端词典覆盖率小于0.3的双语语料
5. 使用过滤后的双语语料的英文单语训练语言模型，根据语言模型筛选提供的英文单语语料，通过back-translation方式生成伪数据，用作数据增强
6. 混合筛选的双语平行语料与生成的伪数据，做去重操作

筛选后，保留了12M的平行语料，4M的伪数据。对训练集进行BPE处理，BPE词表大小为32K，源语端训练词表大小为48K，目标端训练词表大小为33K