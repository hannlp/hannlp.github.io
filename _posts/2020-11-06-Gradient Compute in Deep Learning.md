---
title: Gradient Compute in Deep Learning
date: 2019-04-16 19:32:34
tags:
- 基础知识
- 深度学习
---

# 前言
- *这篇博客的初心* : 最近读的论文又用到LSTM了，发现对这些深度学习模型我还是只了解皮毛，不了解其底层原理(如参数的更新)，而我从接触深度学习开始就对反向传播充满了好奇，感觉这是个很难理解的事情。所以建立这篇博客慢慢从矩阵求导开始，慢慢推导所有深度学习模型的底层原理，从而加深自己的理解。
- *这篇博客内容* : 包括**部分深度学习所需数学知识**，以及**各种深度学习模型(DNN,RNN等)的原理推导**。

# 1 数学知识
> 注: 在本博客中，所有向量<span>$\bm{x}$</span>默认都为列向量

## 1.1 深度学习中几种常见的求导
在神经网络中，很常见的求导类型是一个**实值函数$f$**(如损失函数)对**一个向量$\bm{x}$**(如网络某一层的输出)或**一个矩阵$\bm{W}$**(如网络中的参数)进行求导，这些求导的实质其实就是多元函数求导，即求自变量关于函数值的梯度。

### 1.1.1 常数对向量的求导
$\bm{x}=\begin{pmatrix} x_1 \\ x_2 \\ ... \\x_n \end{pmatrix}$
# 2 深度神经网络中的反向传播推导

# 参考文献
1.[Matrix Cookbook - Kaare Brandt Petersen, Michael Syskind Pedersen](https://cdn.jsdelivr.net/gh/hannlp/Books@1.01/Matrix%20Cookbook.pdf)


