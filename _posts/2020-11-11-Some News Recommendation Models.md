---
title: Some News Recommendation Models NPA/ NAML/ LSTUR/ NRMS
date: 2020-11-11
tags:
- 推荐系统
---

# 前言
上上上次组会研一学长汇报了一篇数据集文章：*MIND: A Large-scale Dataset for News Recommendation*，是微软为**新闻推荐**而发布的一个数据集。在听汇报时我发现这个数据集非常符合我的需求：

1. 首先，新闻推荐需要处理大量的**文本信息**，正与我未来方向(NLP)有较大关联
2. 新闻内容中包含着大量的实体，更有利于探索基于知识(知识图谱)的推荐方法

于是乎，我立马自己去读了这篇[MIND论文(点进去就是其官方网站)](https://msnews.github.io/)，数据格式等就暂不介绍，有兴趣的可以自己去官网查看。在论文中，微软官方实现了几个新闻推荐的算法，如下图:

![rec_models](/imgs/newsrec/rec_models.png)

其中，DKN这篇论文我在去年已经读过并研究过代码了，现在效果比它好的有四个，NPA，NAML，LSTUR和NRMS。我去找来并阅读了这四篇论文，发现第1，2，4篇是同一个人([清华一个很强的博士](https://wuch15.github.io/))发的...而且他也是第3篇的参与者。

本小菜鸡简单的在我的博客里写一下对这四篇论文的分析和理解~出场顺序大概就按照上图中的**效果从低到高**吧^ ^

# 0.几篇论文的共同点

因为这几篇论文的出处差不多，所以共同点非常多。

1. 这四篇论文都是基于三个主要模块：**新闻表示模型**、**用户表示模型**和**点击预测**(包括之前的DKN也是)。其中，*新闻表示模型*通常都从新闻内容(如新闻标题，新闻类别，新闻内容，新闻内容中包含的实体)中学习，而*用户表示*通常从用户的浏览历史新闻中学习，*点击预测*即使用新闻表示和用户表示来计算用户点击这个新闻的概率
2. 这四篇论文都大量使用了*Attention Mechanism*注意力机制(DKN在对用户建模时也使用了)

我在总结的时候，对于共同之处，我会一笔带过，重点关注每篇论文真正创新的地方~

# 1. NPA: Neural News Recommendation with Personalized Attention
## 1.1 核心思想

不同的用户会有不同的兴趣，同时每个用户往往有多种兴趣。所以，不同的用户可能会因为某个新闻的不同方面而点击这个新闻。

两个直观的感觉：

1. 一个新闻标题中的不同单词往往会对用户产生不同的影响
2. 并不是一个用户所浏览过的所有新闻都能反映他的偏好

基于这两个直觉，作者分别提出了*word-level Attention*的**news model**和*news-level Attention*的**user model**~

## 1.2 模型

![npa_model](/imgs/newsrec/npa_model.png)

### 1.2.1 新闻表示模型

新闻表示模型在上图中用绿色虚线圈着。

对于每一个**输入的新闻**(就是其标题文本，一个单词序列)，使用*新闻表示模型*得到最终的**表示向量**。过程大概如下：

1. 词嵌入。即使用词向量(word2vec/glove)技术，将标题中每个单词映射成其对应的向量表示，这样，新闻标题就变成了一个词向量序列$E=(\bm{e_1},...,\bm{e_M})$
2. 卷积神经网络。使用卷积神经网络的目的是想学习每个单词的**局部上下文信息**，也就是使用一个单词和其前后$k$个单词的表示，通过卷积操作来共同学习这个单词的表示。这样，得到一个新的单词表示序列$C=(\bm{c_1},...,\bm{c_M})$
3. **单词级别的注意力机制**，是本篇论文的核心之一。其目的是：为标题中的每个单词，通过注意力机制得到一个权重。这些权重就可以体现用户对每个单词的**关注度**，再将所有单词加权平均，得到新闻标题，也就是新闻的最终表示
![npa_model](/imgs/newsrec/npa_model2.png)

> *3.1* 首先需要将每个用户的ID映射成一个向量$\bm{e_u}$(具体方法文中没有讲，可能是随机初始化最后学习得到的，需要去看代码了解)<br>*3.2* 再将$\bm{e_u}$经过一个线性映射，得到一个查询向量$\bm{q_w}=ReLU(\bm{V_w}\times\bm{e_u}+\bm{v_w})$，其中$\bm{V_w}$和$\bm{v_m}$都是模型参数(就是会不断被更新的辣种)，下面同理!<br>*3.3* 最后通过以下两个公式
$$
\begin{aligned}
    &a_i=\bm{c_i}^\mathrm{T}tanh(\bm{W_p}\times\bm{q_w}+\bm{b_p})\\
    &\alpha_i=\frac{exp(a_i)}{\sum_{j=1}^Mexp(a_j)}
\end{aligned}
$$
即使用查询向量$\bm{q_w}$先经过非线性映射，再与每个单词做点乘，最后使用$Softmax$函数归一化，得到每个单词的注意力。

4. 一个新闻最终的表示，就是所有单词的加权平均啦：$\bm{r_i}=\sum_{j=1}^M\alpha_j\bm{c_j}$

### 1.2.2 用户表示模型
当我写完新闻表示模型后，就大松了一口气，为啥呢！因为这个模型(尤其是**Attention**的思想)在后面被用到了数次...包括马上要说到的用户表示模型！

用户表示模型在上图中用黄色虚线圈着。上文说过，新闻推荐中大家一般都会使用**一个用户浏览过的数条历史新闻**来表示这个用户。如果有$N$个历史新闻，那么就需要使用$N$次新闻表示模型，来得到它们的表示$\{\bm{r_1,...,r_N}\}$

接下来就是似曾相识的模型了，我们知道，在新闻表示模型中，作者用的是**单词级别的注意力机制**，而在用户表示模型中，就要使用**新闻级别的注意力机制了**，由于新闻的表示和单词的表示都是**向量**，所以两部分全部相同。唯一需要注意的是，在计算用户对每条新闻的注意力$\alpha'_j$时，需要生成一个新的查询向量$\bm{q_d}$，而不能继续使用单词attention时的查询向量$\bm{q_w}$。

所以，同样通过所有新闻的加权平均，我们就得到了用户的最终表示：$\bm{u}=\sum_{j=1}^N\alpha'_j\bm{r_j}$

### 1.2.3 点击预测模型
本篇文章还有一个看起来比较出色的点就是他最后的点击预测阶段。

作者指出，以往的评分预测都是使用**一个**新闻表示和**一个**用户表示，在这些方法中，正例新闻和负例新闻往往都是通过随机采样得到的，这就浪费了负例新闻中的很多信息。另外，新闻总量很大，一次一条还是过于慢了。

所以，作者使用了一个负采样的策略。就是一次采样$K+1$个新闻，其中一个正例，$K$个负例。在训练时，当成一个多分类的任务，采用以下公式：
$$
\begin{aligned}
    &\hat y_i'=\bm{r_i'^\mathrm Tu}\\
    &\hat y_i=\frac{exp(\hat y_i')}{\sum_{j=0}^Kexp(\hat y_j')}\\
    &L=-\sum_{y_j\in S}log(\hat y_j)
\end{aligned}
$$
训练目标是在正例新闻集合$S$中，最小化似然函数$L$。使用梯度下降等优化算法，就可以更新模型中的所有参数。另外这种方法还可以将计算复杂度降低到大约$K$分之一。

## 1.3 实验结果和其他有价值的东东
![npa_ex](/imgs/newsrec/npa_ex.png)
![npa_visual_attention](/imgs/newsrec/npa_visual_attention.png)

# 2. NAML: Neural News Recommendation with Attentive Multi-View Learning
## 2.1 核心思想
从新闻的多种成分(标题，类别，内容)中学习到有用的表示(将他们看成新闻的多个视角)

有如下假设：
1. 新闻包含多种成分，对学习其表示有所帮助(我理解为信息可以相互补充)
2. 新闻的不同成分通常会蕴含着不同特性(比如标题很简明扼要，内容会长而具体)
3. 不同用户对不同新闻有不同的注意(同npa模型)

## 2.2 模型
该模型基本与npa相同，只有在**news-model**中，又考虑了另外两个成分(新闻类别和新闻内容)，作为多个**view**。

# 3. LSTUR: Neural News Recommendation with Long- and Short-termUser Representations

# 4. NRMS: Neural News Recommendation with Multi-Head Self-Attention