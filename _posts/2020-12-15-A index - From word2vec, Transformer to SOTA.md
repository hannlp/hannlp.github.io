---
title: A index - From word2vec, Transformer to SOTA 
date: 2020-12-15
tags:
- 自然语言处理
---

# 前言


# 1 词向量
## 1.1 word2vec
## 1.2 ELMo

# 2 预训练(语言)模型
## 2.1 Transformer
![](https://i.loli.net/2020/12/15/kUp6erNM2tAZ4zH.png)
![](https://i.loli.net/2020/12/15/ZGCuHEVtlbUd1ap.png)
### 2.1.1 一段话总结
### 2.1.2 模型源码
1. [attention-is-all-you-need-pytorch - jadore801120](https://github.com/jadore801120/attention-is-all-you-need-pytorch) (pytorch版本，**首推**，无其他冗余代码)
2. [The Annotated Transformer - harvardnlp](https://nlp.seas.harvard.edu/2018/04/03/attention.html) (pytorch版本，哈佛大学)
3. [Trax — Deep Learning with Clear Code and Speed](https://github.com/google/trax) (tensorflow版本，**官方实现**，有较多冗余代码)

### 2.1.3 优质文章索引
1. [Attention? Attention! - Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
2. [The Illustrated Transformer - jalammar](https://jalammar.github.io/illustrated-transformer/)
3. [《Attention is All You Need》浅读（简介+代码）- 苏剑林](https://kexue.fm/archives/4765)
4. [深入理解Transformer及其源码 - ZingpLiu](https://www.cnblogs.com/zingp/p/11696111.html)

## 2.2 Bert
![](https://i.loli.net/2020/12/15/18wZPMjQp5COuT2.png)
![](https://i.loli.net/2020/12/15/U4htoOYcn1kLTAy.png)
### 2.2.1 一段话总结
### 2.2.2 模型源码
1. [BERT-pytorch - codertimo](https://github.com/codertimo/BERT-pytorch) (pytorch版本，**首推**)
2. [bert - google-research](https://github.com/google-research/bert) (tensorflow版本，**官方实现**，附[源码解析 - 梁源](https://www.cnblogs.com/Milburn/p/12031521.html))
### 2.1.3 优质文章索引
1. [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) - Jay Alammar](https://jalammar.github.io/illustrated-bert/)
2. [一文读懂BERT(原理篇) - 忧郁得茄子
](https://blog.csdn.net/jiaowoshouzi/article/details/89073944)
3. [谷歌BERT预训练源码解析 - 保持一份率性
](https://blog.csdn.net/weixin_39470744)
4. [关于BERT的若干问题整理记录 - Adherer
](https://zhuanlan.zhihu.com/p/95594311)
