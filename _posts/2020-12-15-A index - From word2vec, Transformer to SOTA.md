---
title: A index - From word2vec, Transformer to SOTA 
date: 2020-12-15
tags:
- 自然语言处理
---

# 前言


# 1 词向量
## 1.1 word2vec
## 1.2 ELMo

# 2 预训练(语言)模型
## 2.1 Transformer
![](https://i.loli.net/2020/12/15/xYmWtFRQnkJcGv8.png)
### 2.1.1 一段话总结
### 2.1.2 模型源码
1. [attention-is-all-you-need-pytorch - jadore801120](https://github.com/jadore801120/attention-is-all-you-need-pytorch) (pytorch版本，**首推**，无其他冗余代码)
2. [The Annotated Transformer - harvardnlp](https://nlp.seas.harvard.edu/2018/04/03/attention.html) (pytorch版本，哈佛大学)
3. [Trax — Deep Learning with Clear Code and Speed](https://github.com/google/trax) (tensorflow版本，**官方实现**，有较多冗余代码)

### 2.1.3 优质文章索引
1. [Attention? Attention! - Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
2. [The Illustrated Transformer - jalammar](https://jalammar.github.io/illustrated-transformer/)
3. [《Attention is All You Need》浅读（简介+代码）- 苏剑林](https://kexue.fm/archives/4765)
4. [深入理解Transformer及其源码 - ZingpLiu](https://www.cnblogs.com/zingp/p/11696111.html)